{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Variable Selection for Uncorrelated Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "\n",
    "from arviz import summary, plot_trace\n",
    "from theano.tensor import _shared\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncorrelated Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Data\n",
    "k = 20 # number of predictors\n",
    "p = 5 # number of nonzero predictors\n",
    "n = 1000\n",
    "sigma = 0.5\n",
    "\n",
    "np.random.seed(615)\n",
    "\n",
    "zero_idx = np.random.choice(np.arange(0, k), k-p, replace=False)\n",
    "true_beta = np.random.randn(k)\n",
    "true_beta[zero_idx] = 0\n",
    "\n",
    "# Predictor variable\n",
    "X = pm.MvNormal.dist(mu=np.zeros(k), cov=np.diag(np.ones(k)), shape=[1, k]).random(size=n)\n",
    "\n",
    "# Simulate outcome variable\n",
    "y = X.dot(true_beta) + np.random.randn(n) * sigma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-1.02976882  0.          0.         -0.22188525  0.44980561  0.\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          1.05456722  0.          0.\n -1.06592054  0.        ]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-1.01714618, -0.01625893, -0.00351084, -0.28006783,  0.44542211,\n       -0.01220991, -0.00860135,  0.01057332,  0.0083056 ,  0.0197452 ,\n        0.0010527 ,  0.00515986,  0.03849674, -0.01107302,  0.01349936,\n        1.04489314, -0.01098878, -0.01893812, -1.04890962, -0.02010866])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "print(true_beta)\n",
    "np.linalg.inv(X.T@X)@X.T@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "y = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()\n",
    "X = scaler_x.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-1.02976882  0.          0.         -0.22188525  0.44980561  0.\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          1.05456722  0.          0.\n -1.06592054  0.        ]\n[-5.26563750e-01 -8.59651521e-03 -1.25043432e-03 -1.43678214e-01\n  2.30349046e-01 -6.60513978e-03 -4.37155848e-03  5.71848680e-03\n  4.73624392e-03  9.81960623e-03  5.19403435e-04  2.57015908e-03\n  2.08522125e-02 -5.58917609e-03  6.99037300e-03  5.34373482e-01\n -6.12025628e-03 -9.97442295e-03 -5.55034609e-01 -1.12577663e-02]\n[-1.09385827  0.07551868 -0.07061976 -0.27297455  0.46413591  0.06688127\n -0.00492034 -0.0056113  -0.00249954  0.1095043   0.06013847  0.06652993\n  0.06661442  0.02227378  0.01937467  1.02996563  0.09947127  0.02078644\n -1.01952879  0.18149412]\n"
    }
   ],
   "source": [
    "print(true_beta)\n",
    "print(np.linalg.inv(X.T@X)@X.T@y)\n",
    "print(scaler_y.inverse_transform(scaler_x.inverse_transform(np.linalg.inv(X.T@X)@X.T@y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = _shared(X)\n",
    "y_obs = _shared(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "with pm.Model() as lm_model:\n",
    "    # Priors\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=5, shape=k)\n",
    "    sd = pm.Gamma(\"sd\", alpha=0.05, beta=0.1, shape=1)\n",
    "    \n",
    "    # Likelihood\n",
    "    mu = pm.Deterministic(\"mu\", tt.dot(X, beta))\n",
    "    y_ = pm.Normal(\"y_obs\", mu=mu, sd=sd, observed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.check_test_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model1:\n",
    "    tr = pm.sample(2000, tune=1000, cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines= [(\"beta\", {\"beta_dim_0\": i}, true_beta[i]) for i in range(0, k)]\n",
    "fig1 = plot_trace(tr, var_names=[\"beta\"], lines=lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike and Slab Prior - George and McCulloch 1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model2:\n",
    "    # Spike and Slab Priors\n",
    "    spike_prior = pm.Normal.dist(mu=0, sigma=0.01)\n",
    "    slab_prior = pm.Normal.dist(mu=0, sigma=100)\n",
    "    \n",
    "    # w = pm.Bernoulli(\"w\", p=0.25, shape=k) # Multinomial\n",
    "    w = pm.Beta(\"w\", alpha=0.25, beta=0.75, shape=k)\n",
    "    weights = pm.Deterministic(\"weights\", pm.math.stack([1.-w, w], axis=1))\n",
    "\n",
    "    # Independent Normal Priors\n",
    "    beta = pm.Mixture(\"beta\", w=weights, comp_dists=[spike_prior, slab_prior], shape=k)\n",
    "    sd = pm.Gamma(\"sd\", alpha=0.05, beta=0.1)\n",
    "\n",
    "    # Likelihood\n",
    "    mu = pm.Deterministic(\"mu\", tt.dot(x, beta))\n",
    "    y_ = pm.Normal(\"y_\", mu=mu, sd=sd, observed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.check_test_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model2:\n",
    "    tr = pm.sample(2000, tune=1000, cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_beta)\n",
    "summary(tr, var_names=[\"beta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_beta)\n",
    "fig1 = plot_trace(tr, var_names=[\"w\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model3:\n",
    "    \n",
    "    # Priors\n",
    "    sd = pm.Gamma(\"sd\", alpha=0.05, beta=0.1, shape=1)\n",
    "    tau = pm.HalfCauchy(\"tau\", beta=1, shape=1) # global shrinkage\n",
    "    lam = pm.HalfCauchy(\"lam\", beta=1/s, shape=k) # local shrinkage\n",
    "\n",
    "    scale = pm.Deterministic(\"scale\", pm.math.sqr(lam*tau))\n",
    "    beta_tilde = pm.Normal(\"beta_tilde\", mu=0, sigma=scale, shape=k)\n",
    "    beta = pm.Deterministic(\"beta\", beta_tilde*tau*lam*sd)\n",
    "    \n",
    "    kappa = pm.Deterministic(\"kappa\", 1/(1+pm.math.sqr(lam)))\n",
    "\n",
    "    # Likelihood\n",
    "    mu = pm.Deterministic(\"mu\", tt.dot(x, beta))\n",
    "    y_ = pm.Normal(\"y_obs\", mu=mu, sd=sd, observed=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.check_test_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model3:\n",
    "    step = pm.NUTS(target_accept=0.95, max_treedepth=15, t0=15)\n",
    "    tr = pm.sample(1000, tune=1000, cores=4, chains=8, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(tr, var_names=[\"kappa\", \"beta\", \"sd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines= [(\"beta\", {\"beta_dim_0\": i}, true_beta[i]) for i in range(0, k)]\n",
    "fig1 = plot_trace(tr, var_names=[\"beta\"], lines=lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finnish Horseshoe Prior - Piironen and Vehtari 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "m0 = p  # expected number of nonzero predictors\n",
    "M = k\n",
    "N = n\n",
    "sigma = np.var(y)\n",
    "tau0 = m0/(M-m0)*sigma/np.sqrt(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 25.\n",
    "s_sq = 1\n",
    "\n",
    "with pm.Model() as model4:\n",
    "    \n",
    "    # Priors\n",
    "    sd = pm.Gamma(\"sd\", alpha=0.05, beta=0.1, shape=1)\n",
    "    # sd = pm.Normal(\"sd\", mu=0.05, sd=2)\n",
    "\n",
    "    # Hyperpriors\n",
    "\n",
    "    tau = pm.HalfCauchy(\"tau\", beta=tau0, shape=1) \n",
    "    c_sq = pm.InverseGamma(\"c_sq\", alpha=nu/2, beta=nu/2*s_sq, shape=1)\n",
    "    lam = pm.HalfCauchy(\"lam\", beta=1, shape=k)     \n",
    "    lam_tilde = pm.Deterministic(\"lam_tilde\", pm.math.sqrt(c_sq)*lam/pm.math.sqrt(c_sq + pm.math.sqr(tau*lam)))\n",
    "\n",
    "    beta_tilde = pm.Normal(\"beta_tilde\", mu=0, sd=1, shape=k)\n",
    "    beta = pm.Deterministic(\"beta\", tau*lam_tilde*beta_tilde)\n",
    "    \n",
    "    # Likelihood\n",
    "    mu = pm.Deterministic(\"mu\", tt.dot(x, beta))\n",
    "    y_ = pm.Normal(\"y_obs\", mu=mu, sd=sd, observed=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.check_test_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model4:\n",
    "    tr = pm.sample(2000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_beta)\n",
    "summary(tr, var_names=[\"beta\", \"sd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_beta)\n",
    "fig1 = plot_trace(tr, var_names=[\"beta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Data\n",
    "k = 20\n",
    "n = 1000\n",
    "sigma = 0.5\n",
    "\n",
    "np.random.seed(615)\n",
    "true_beta = np.random.randn(k)\n",
    "true_beta[[2 ,3, 5, 7, 8, 9, 10, 11, 12, 13,  15, 16, 17, 19]] = 0\n",
    "\n",
    "L = np.array(\n",
    "    [[1., 0., 0., 0., 0., 0.],\n",
    "    [0.5, 0.4, 0., 0., 0., 0.],\n",
    "    [0.2, 0.5 , -0.7, 0., 0., 0.],\n",
    "    [0., 0., 0., 1., 0., 0.],\n",
    "    [0.7, 0.06, 0.03, 0., 0.8, 0.],\n",
    "    [-0.7, 0., 0., 0., 0.08, .1]]\n",
    "    )\n",
    "K = L@L.T\n",
    "\n",
    "# Predictor variable\n",
    "#X = pm.MvNormal.dist(mu=np.zeros(6), cov=K, shape=[1, 6]).random(size=n)\n",
    "X = pm.MvNormal.dist(mu=np.zeros(k), cov=np.diag(np.ones(k)), shape=[1, k]).random(size=n)\n",
    "x = _shared(X)\n",
    "\n",
    "# Simulate outcome variable\n",
    "y = X.dot(true_beta) + np.random.randn(n) * sigma \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.zeros(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[np.where(y > 0)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "m0 = k -2  # expected number of relevant slopes\n",
    "M = k\n",
    "N = n\n",
    "sigma = 1/(np.mean(y)*(1-np.mean(y)))\n",
    "tau0 = m0/(M-m0)*sigma/np.sqrt(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "m0 = k -2  # expected number of relevant slopes\n",
    "M = k\n",
    "N = n\n",
    "sigma = 1/(np.mean(y)*(1-np.mean(y)))\n",
    "tau0 = m0/(M-m0)*sigma/np.sqrt(N)\n",
    "nu = 25.\n",
    "s_sq = 1\n",
    "\n",
    "with pm.Model() as logit:\n",
    "    \n",
    "    # Priors\n",
    "    sd = pm.Gamma(\"sd\", alpha=0.05, beta=0.1, shape=1)\n",
    "    # sd = pm.Normal(\"sd\", mu=0.05, sd=2)\n",
    "\n",
    "    # Hyperpriors\n",
    "    tau = pm.HalfCauchy(\"tau\", beta=tau0, shape=1) \n",
    "    c_sq = pm.InverseGamma(\"c_sq\", alpha=nu/2, beta=nu/2*s_sq, shape=1)\n",
    "    lam = pm.HalfCauchy(\"lam\", beta=1, shape=k)     \n",
    "    lam_tilde = pm.Deterministic(\"lam_tilde\", pm.math.sqrt(c_sq)*lam/pm.math.sqrt(c_sq + pm.math.sqr(tau*lam)))\n",
    "\n",
    "    beta_tilde = pm.Normal(\"beta_tilde\", mu=0, sd=1, shape=k)\n",
    "    beta = pm.Deterministic(\"beta\", tau*lam_tilde*beta_tilde)\n",
    "    \n",
    "    # Likelihood\n",
    "    mu = pm.Deterministic(\"mu\", tt.dot(x, beta))\n",
    "    p = pm.Deterministic('p', pm.math.exp(pm.Normal.dist(0, 1).logcdf(mu)))\n",
    "    y_ = pm.Bernoulli('y', p=p, observed=z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.check_test_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "with logit:\n",
    "    tr = pm.sample(2000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(tr, var_names=[\"beta\", \"sd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_trace(tr, var_names=[\"beta\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logit:\n",
    "    ppc = pm.sample_posterior_predictive(tr, samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_samps = ppc['y']\n",
    "z_pred = np.zeros(z_pred_samps.shape[1])\n",
    "z_probs = np.zeros([z_pred.shape[0], 2])\n",
    "\n",
    "for i in range(0, len(y)):\n",
    "\n",
    "    p1 = np.mean(z_pred_samps[:,i] == 0)\n",
    "    p2 = np.mean(z_pred_samps[:,i] == 1)\n",
    "    probs = [p1, p2]\n",
    "    z_probs[i] = probs\n",
    "    z_pred[i] = probs.index(max(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(z, z_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy:{round(accuracy_score(y_true=z, y_pred=z_pred),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitbayesianmethodspipenv46152ad7b29540c2b32faa0e8d5795a9",
   "display_name": "Python 3.7.7 64-bit ('bayesian_methods': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}